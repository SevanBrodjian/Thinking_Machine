# Stream of Consciousness LLM

The purpose of this project was to experiment with fine-tuning Large Language Models (LLMs) to mimic the stream of consciousness generated by humans. The goal was to investigate if LLMs could be adequately trained to produce interesting emergent phenomena, such as self-references or an internal state.

## Data Collection and Preparation

Approximately 450 sentences of personal streams of consciousness were manually gathered over several days and hours of typing. These sentences covered various subjects and were split into 4-sentence chunks, forming the "response" portion of prompt-response pairs for fine-tuning. Each 4-sentence chunk was processed by GPT-3 to generate a summary, which was then crafted into a prompt of the form "Think about...". In total, over 100 prompt-response pairs were generated.

## Fine-tuning and Model Selection

The chosen model for fine-tuning was the OpenAI Davinci model. The process of fine-tuning the model cost approximately $13. After the fine-tuning process, a personal model was developed that could be used to generate new thoughts.

## Results and Observations

As expected, the model started losing coherence after approximately 4 sentences, as it deviated from the training data. However, it consistently produced thought-provoking responses in short formats. The model struggled to generate thoughts on topics outside its training data, primarily focusing on existential observations.

One unforeseen result was that the model occasionally and consistently assumed the identity of the project creator, referring to itself as "Sevan." Moreover, many of the generated thoughts were more personal and resembled the creator's style, leading friends and family to comment that it sounded like the creator. Inspired by this, a new project was initiated to train an LLM on over 10,000 prompt-response pairs of actual text messages the creator had with friends. However, this attempt did not yield satisfactory results.

## Conclusion

It is my opinion that models of today are not capable of generating realistic streams of consciousness, even with improvements in the prompt-response pairs. However, this marks the early stage of stream of consciousness LLMs, and I foresee remarkably realistic thought patterns being generated just one or two iterations down the line.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details
